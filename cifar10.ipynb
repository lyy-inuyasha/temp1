{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10('/home/hefb/data/', train=True, transform=transform_train)\n",
    "valset = torchvision.datasets.CIFAR10('/home/hefb/data', train=False, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32]) 6\n"
     ]
    }
   ],
   "source": [
    "image, label = trainset[0]\n",
    "print(image.shape, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.bn(self.conv(x)))\n",
    "\n",
    "\n",
    "class ResBlock2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, stride=1):\n",
    "        super().__init__()\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.downsample = None\n",
    "        if out_channels != in_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride), \n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        out = F.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        mid_channels = in_channels // expansion\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "\n",
    "        self.downsample = None\n",
    "        if out_channels != in_channels or stride != 1:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride), \n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, 3, stride, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv3 = nn.Conv2d(mid_channels, out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(identity)\n",
    "        \n",
    "        x += identity\n",
    "        out = F.relu(x)\n",
    "        return out\n",
    "        \n",
    "class ResNetBackbone2d(nn.Module):\n",
    "    def __init__(self, in_channels=3, layers=[2,2,2,2], channels=[64,128,256,512], dropout=0, block=ResBlock2d):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels[0], 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
    "\n",
    "        self.layers = []\n",
    "        pre_chns = channels[0]\n",
    "        strides = [1, 2, 2, 2]\n",
    "        for num_blocks, num_channels, stride in zip(layers, channels, strides):\n",
    "            self.layers.append(ResNetBackbone2d._make_layer(block, num_blocks, pre_chns, num_channels, stride))\n",
    "            pre_chns = num_channels\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bs = x.shape[0]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.layers(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gap(x)\n",
    "\n",
    "        return x.reshape(bs, -1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(block, num_blocks, in_channels, out_channels, stride=1):\n",
    "        blocks = []\n",
    "        blocks.append(block(in_channels, out_channels, stride))\n",
    "        for i in range(1, num_blocks):\n",
    "            blocks.append(block(out_channels, out_channels, 1))\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "\n",
    "class ResNet2d(nn.Module):\n",
    "    def __init__(self, num_classes=10, layers=[2,2,2,2], channels=[64,128,256,512], dropout=0, block=ResBlock2d):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetBackbone2d(3, layers, channels, dropout, block)\n",
    "        self.classifier = nn.Linear(channels[-1], num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        out = self.classifier(feat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "train_loader = DataLoader(trainset, 32, num_workers=4, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(valset, 32, num_workers=4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = ResNet2d(10, [2,2,2,2], [64,128,256,512]).to('cuda:3')\n",
    "optimizer = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 | acc:0.3597351312637329, loss:1.7293163936361002\n",
      "Test Epoch 0 | acc:0.47849997878074646, loss:1.4034170670509338\n",
      "Train Epoch 1 | acc:0.5630801916122437, loss:1.2084662023259185\n",
      "Test Epoch 1 | acc:0.6014999747276306, loss:1.1270315172195435\n",
      "Train Epoch 2 | acc:0.6777768731117249, loss:0.9124075979299643\n",
      "Test Epoch 2 | acc:0.7379999756813049, loss:0.7551484267234803\n",
      "Train Epoch 3 | acc:0.7483394742012024, loss:0.7268371815947046\n",
      "Test Epoch 3 | acc:0.7759999632835388, loss:0.6545242427825928\n",
      "Train Epoch 4 | acc:0.7887123823165894, loss:0.6087451917566502\n",
      "Test Epoch 4 | acc:0.8141999840736389, loss:0.5315461729526519\n",
      "Train Epoch 5 | acc:0.8150408267974854, loss:0.5354676114278398\n",
      "Test Epoch 5 | acc:0.8156999945640564, loss:0.5348099000930786\n",
      "Train Epoch 6 | acc:0.834326982498169, loss:0.47981268288174145\n",
      "Test Epoch 6 | acc:0.8345999717712402, loss:0.48754143226146696\n",
      "Train Epoch 7 | acc:0.8470910787582397, loss:0.4405014265314336\n",
      "Test Epoch 7 | acc:0.8585999608039856, loss:0.42457300536632536\n",
      "Train Epoch 8 | acc:0.8626760840415955, loss:0.4005801230192032\n",
      "Test Epoch 8 | acc:0.8518999814987183, loss:0.4460124598264694\n",
      "Train Epoch 9 | acc:0.8733595013618469, loss:0.3722586963235111\n",
      "Test Epoch 9 | acc:0.851099967956543, loss:0.4409068447113037\n",
      "Train Epoch 10 | acc:0.8805017471313477, loss:0.34823585773142063\n",
      "Test Epoch 10 | acc:0.8434999585151672, loss:0.4555601946353912\n",
      "Train Epoch 11 | acc:0.8881441950798035, loss:0.3236876131880852\n",
      "Test Epoch 11 | acc:0.8658999800682068, loss:0.40274391937851906\n",
      "Train Epoch 12 | acc:0.8953865170478821, loss:0.304089793894874\n",
      "Test Epoch 12 | acc:0.8750999569892883, loss:0.3806399766981602\n",
      "Train Epoch 13 | acc:0.9013684391975403, loss:0.28611664914510543\n",
      "Test Epoch 13 | acc:0.8728999495506287, loss:0.40100934689044954\n",
      "Train Epoch 14 | acc:0.908290684223175, loss:0.2634751980482731\n",
      "Test Epoch 14 | acc:0.8707000017166138, loss:0.4065940823733807\n",
      "Train Epoch 15 | acc:0.9097511172294617, loss:0.2585257175503711\n",
      "Test Epoch 15 | acc:0.8863999843597412, loss:0.3598699765443802\n",
      "Train Epoch 16 | acc:0.9169134497642517, loss:0.23618648644350707\n",
      "Test Epoch 16 | acc:0.8804000020027161, loss:0.39452058714032173\n",
      "Train Epoch 17 | acc:0.9212948083877563, loss:0.22855467406112256\n",
      "Test Epoch 17 | acc:0.8858000040054321, loss:0.37422107863128184\n",
      "Train Epoch 18 | acc:0.9232754707336426, loss:0.2150806505274786\n",
      "Test Epoch 18 | acc:0.886199951171875, loss:0.3699706820815802\n",
      "Train Epoch 19 | acc:0.9284971356391907, loss:0.2047173829979374\n",
      "Test Epoch 19 | acc:0.8995999693870544, loss:0.3222660940527916\n",
      "Train Epoch 20 | acc:0.9310779571533203, loss:0.1964474784719511\n",
      "Test Epoch 20 | acc:0.8995999693870544, loss:0.3414766021758318\n",
      "Train Epoch 21 | acc:0.9331586360931396, loss:0.1920458685088588\n",
      "Test Epoch 21 | acc:0.8833000063896179, loss:0.4147458827242255\n",
      "Train Epoch 22 | acc:0.9373399615287781, loss:0.17845484101011985\n",
      "Test Epoch 22 | acc:0.9014999866485596, loss:0.32912717008292675\n",
      "Train Epoch 23 | acc:0.939960777759552, loss:0.1686845571881878\n",
      "Test Epoch 23 | acc:0.8910999894142151, loss:0.36800534390211104\n",
      "Train Epoch 24 | acc:0.9427617192268372, loss:0.16580626332569537\n",
      "Test Epoch 24 | acc:0.9023000001907349, loss:0.3661788123756647\n",
      "Train Epoch 25 | acc:0.9447823166847229, loss:0.1570176022957829\n",
      "Test Epoch 25 | acc:0.8946999907493591, loss:0.3667500592201948\n",
      "Train Epoch 26 | acc:0.946002721786499, loss:0.15473405193035442\n",
      "Test Epoch 26 | acc:0.8919999599456787, loss:0.3751194562792778\n",
      "Train Epoch 27 | acc:0.949583888053894, loss:0.14273701697482433\n",
      "Test Epoch 27 | acc:0.901699960231781, loss:0.3644146551132202\n",
      "Train Epoch 28 | acc:0.9513444304466248, loss:0.13942081200293707\n",
      "Test Epoch 28 | acc:0.8992999792098999, loss:0.3853651730209589\n",
      "Train Epoch 29 | acc:0.9533650875091553, loss:0.13359689654465812\n",
      "Test Epoch 29 | acc:0.8973000049591064, loss:0.398188012740016\n",
      "Train Epoch 30 | acc:0.9541453123092651, loss:0.1322261769138664\n",
      "Test Epoch 30 | acc:0.8948999643325806, loss:0.3983224227681756\n",
      "Train Epoch 31 | acc:0.9563660621643066, loss:0.12498432826745669\n",
      "Test Epoch 31 | acc:0.8916999697685242, loss:0.4093836982101202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7ccb08db777c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epoch = 100\n",
    "for i in range(epoch):\n",
    "    # train epoch\n",
    "    acc, losses, total = 0, 0, 0\n",
    "    model.train()\n",
    "    for image, label in train_loader:\n",
    "        image, label = image.to('cuda:3'), label.to('cuda:3')\n",
    "        output = model(image)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = image.shape[0]\n",
    "        total += bs\n",
    "        losses += loss.item() * bs\n",
    "        acc += torch.sum(label == (output.argmax(dim=1)))\n",
    "    acc = acc / total \n",
    "    losses = losses / total\n",
    "    print(f\"Train Epoch {i} | acc:{acc}, loss:{losses}\")\n",
    "\n",
    "    # val epoch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        acc, losses, total = 0, 0, 0\n",
    "        for image, label in val_loader:\n",
    "            image, label = image.to('cuda:3'), label.to('cuda:3')\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            bs = image.shape[0]\n",
    "            total += bs\n",
    "            losses += loss.item() * bs\n",
    "            acc += torch.sum(label == (output.argmax(dim=1)))\n",
    "        acc = acc / total \n",
    "        losses = losses / total\n",
    "        print(f\"Test Epoch {i} | acc:{acc}, loss:{losses}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1683511cae9047599d842d538876a5a4ba958fd139a3e93d61473731db816d61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
